{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "012f0a7c",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<br/><font size=\"6\">Corpus Batch Processing : Twitter Sentiment140 1.8M tweets</font>    \n",
    "<br/><font size=\"5\">Optimize NLP preprocessing pipeline with batch a processing technique.</font>    \n",
    "<br/><font size=\"4\">Preprocess millions of sentences in a few hours instead of tens.</font>    \n",
    "<br/>(c) Tristan Vanrullen 2022<br/>\n",
    "</div>\n",
    "\n",
    "- [Github source code and notebooks](https://github.com/TristanV/open_studies)\n",
    "- [Medium.com](https://tristanv.medium.com/)\n",
    "- [LinkedIn](https://www.linkedin.com/in/tristan-vanrullen/)\n",
    "- [ResearchGate](https://www.researchgate.net/profile/Tristan-Vanrullen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f066aca",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Introduction\n",
    "\n",
    "The purpose of this notebook is to **compare execution time** for several NLP preprocessing techniques, on a large *sentences-millionnaire* dataset .\n",
    "\n",
    "- We first observe run time of a NLP pipeline, over several subsets of the dataset, to get some insights about the pipeline complexity.\n",
    "- Then we propose a method to compute the optimal batch number to obtain the better batch-processing time.\n",
    "- Finally we use this optimal batch number in an optimized batch-processing loop over the same NLP pipeline. \n",
    "\n",
    "Remark : In this notebook, because we run pipelines for the only purpose to run and benchmark pipelines, we do not really have to build a robust and efficient pipeline covering all NLP aspects! For this reason, we will focus on a few regex transformations, and POS Tagging + Lemmatizing algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136aa099",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1f62929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tristan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load some libraries we may need\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "import string \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline  \n",
    " \n",
    "from IPython.display import display, Image, HTML, clear_output  \n",
    "\n",
    "\n",
    "from os.path import exists\n",
    "from os import remove\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re \n",
    "import nltk  \n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize \n",
    "from nltk.tag.util import untag  \n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b3e1d",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "\n",
    "Let's work with the [Tweets Sentiment140 corpus](http://help.sentiment140.com/).\n",
    "\n",
    "License : no licence provided by Twitter nor the dataset authors for commercial use (2022-02-01)\n",
    "\n",
    "There is no clue that this dataset is provided with Twitter's agreement\n",
    "- see this answer related to the Sentiment140 dataset disclosure : https://groups.google.com/g/sentiment140/c/lu9WuBulUhw?pli=1 \n",
    "- and see Twitter T&Cs : https://developer.twitter.com/en/developer-terms/commercial-terms\n",
    "\n",
    "Many academic publications are available about this dataset, which is distributed by Tensorflow and HuggingFace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32162b4-79de-4390-a021-062c49c22c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=\"./dataset/sentiment140/\"\n",
    "corpus_filename='training.1600000.processed.noemoticon.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2aabc8c-52dd-4901-be8d-e3bd9ad6982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the documents dataset ./dataset/sentiment140/training.1600000.processed.noemoticon.csv  Loading time =  3.810781717300415 s\n"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "documents = pd.read_csv(dataset_path+corpus_filename, encoding = 'latin1', delimiter=\",\", names=cols) # encoding='utf-8'\n",
    "documents.drop(columns=['sentiment','date', 'query_string','user'], axis=1, inplace=True) \n",
    "\n",
    "print('Reading the documents dataset',dataset_path+corpus_filename,' Loading time = ',time()-t0,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39cf5a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810369</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>2193601966</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>2193601969</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>2193601991</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>2193602064</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>2193602129</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               text\n",
       "0        1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1        1467810672  is upset that he can't update his Facebook by ...\n",
       "2        1467810917  @Kenichan I dived many times for the ball. Man...\n",
       "3        1467811184    my whole body feels itchy and like its on fire \n",
       "4        1467811193  @nationwideclass no, it's not behaving at all....\n",
       "...             ...                                                ...\n",
       "1599995  2193601966  Just woke up. Having no school is the best fee...\n",
       "1599996  2193601969  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997  2193601991  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998  2193602064  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999  2193602129  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87e32bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   id      1600000 non-null  int64 \n",
      " 1   text    1600000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "documents.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63814d-7cab-4f26-8af7-db04aec88b3f",
   "metadata": {},
   "source": [
    "## Focus on text\n",
    "\n",
    "In the present study, we are just interested in processing the documents texts with a NLP pipeline, so let's remove all other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95616741-d04e-4d75-8bdb-773a575fbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = documents[['id','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d766ff-5371-4332-9632-7ffb9fce543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   id      1600000 non-null  int64 \n",
      " 1   text    1600000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "documents.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f877f2d8-8c7e-4306-9e58-c29575f8be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_initial_working_file = dataset_path+\"documents_txt.pkl\"\n",
    "documents.to_pickle(documents_initial_working_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf91bc",
   "metadata": {},
   "source": [
    "## Quick data study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c77d530-114b-4cdc-893d-dcedb6642b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents['length']=documents[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4deccc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---  ---  ---  ---  --- \n",
      "Shortest documents : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1071086</th>\n",
       "      <td>1966267712</td>\n",
       "      <td>Yes</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209110</th>\n",
       "      <td>1973889928</td>\n",
       "      <td>ouch</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310554</th>\n",
       "      <td>2001120036</td>\n",
       "      <td>Ugh</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050599</th>\n",
       "      <td>1960762577</td>\n",
       "      <td>hey</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>1468771833</td>\n",
       "      <td>just</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id    text  length\n",
       "1071086  1966267712  Yes          6\n",
       "209110   1973889928  ouch         6\n",
       "310554   2001120036  Ugh          6\n",
       "1050599  1960762577  hey          6\n",
       "4233     1468771833   just        6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---  ---  ---  ---  --- \n",
      "Longest documents:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>650945</th>\n",
       "      <td>2237729497</td>\n",
       "      <td>human shield à®à®©à¯?à®± à®à¯à®±à¯?à®¤à¯à®...</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582941</th>\n",
       "      <td>2190374813</td>\n",
       "      <td>5 days till new top gear  î?î?î?î?î?î?î?...</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380539</th>\n",
       "      <td>2052585042</td>\n",
       "      <td>@neospace à¹à¸£à¸·à¹à¸­à¸à¸à¸±à¹à¸à¸à¸£...</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484882</th>\n",
       "      <td>2067748792</td>\n",
       "      <td>@iannnnn à¹à¸à¸µà¹à¸¢à¸§à¸­à¸µà¸?à¸à¸±à¸?à...</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295281</th>\n",
       "      <td>2003659618</td>\n",
       "      <td>@catfish_ohm à¹à¸à¹à¸²à¹à¸«à¹à¸?à¸²à¸£à¹...</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               text  length\n",
       "650945   2237729497  human shield à®à®©à¯?à®± à®à¯à®±à¯?à®¤à¯à®...     348\n",
       "1582941  2190374813  5 days till new top gear  î?î?î?î?î?î?î?...     359\n",
       "380539   2052585042  @neospace à¹à¸£à¸·à¹à¸­à¸à¸à¸±à¹à¸à¸à¸£...     359\n",
       "1484882  2067748792  @iannnnn à¹à¸à¸µà¹à¸¢à¸§à¸­à¸µà¸?à¸à¸±à¸?à...     369\n",
       "1295281  2003659618  @catfish_ohm à¹à¸à¹à¸²à¹à¸«à¹à¸?à¸²à¸£à¹...     374"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" --- \"*5)\n",
    "print(\"Shortest documents : \")\n",
    "display(documents.sort_values(by=\"length\").head())\n",
    "print()\n",
    "print(\" --- \"*5)\n",
    "print(\"Longest documents:\")\n",
    "display(documents.sort_values(by=\"length\").tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd06389",
   "metadata": {},
   "source": [
    "## Emojis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee6c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict = {\n",
    "\":)\": \"_emo_pos_\",\n",
    "\":‑)\": \"_emo_pos_\",\n",
    "\":-]\": \"_emo_pos_\",\n",
    "\":-3\": \"_emo_pos_\",\n",
    "\":->\": \"_emo_pos_\",\n",
    "\"8-)\": \"_emo_pos_\",\n",
    "\":-}\": \"_emo_pos_\",\n",
    "\":o)\": \"_emo_pos_\",\n",
    "\":c)\": \"_emo_pos_\",\n",
    "\":^)\": \"_emo_pos_\",\n",
    "\"=]\": \"_emo_pos_\",\n",
    "\"=)\": \"_emo_pos_\",\n",
    "\"<3\": \"_emo_pos_\",\n",
    "\":-(\": \"_emo_neg_\",\n",
    "\":(\": \"_emo_neg_\",\n",
    "\":c\": \"_emo_neg_\",\n",
    "\":<\": \"_emo_neg_\",\n",
    "\":[\": \"_emo_neg_\",\n",
    "\">:[\": \"_emo_neg_\",\n",
    "\":{\": \"_emo_neg_\",\n",
    "\">:(\": \"_emo_neg_\",\n",
    "\":-c\": \"_emo_neg_\",\n",
    "\":-< \": \"_emo_neg_\",\n",
    "\":-[\": \"_emo_neg_\",\n",
    "\":-||\": \"_emo_neg_\"\n",
    "}\n",
    "# display(emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bbd0c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_dict(text, dictionary):\n",
    "    for key,value in dictionary.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08c54335",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents['emoji_text'] = documents['text'].apply(lambda x: lookup_dict(x,emoji_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdfad93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents containing positive emoji: 660\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>1467986323</td>\n",
       "      <td>Re-pinging @Kyle44: Custom icons I made! =] lo...</td>\n",
       "      <td>125</td>\n",
       "      <td>Re-pinging @Kyle44: Custom icons I made! _emo_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>1468472273</td>\n",
       "      <td>@swati121 SWATI!!! omg i missed you soo much  ...</td>\n",
       "      <td>125</td>\n",
       "      <td>@swati121 SWATI!!! omg i missed you soo much  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10129</th>\n",
       "      <td>1550780676</td>\n",
       "      <td>its a sunny day today yay =] but still cold</td>\n",
       "      <td>44</td>\n",
       "      <td>its a sunny day today yay _emo_pos_ but still ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12206</th>\n",
       "      <td>1551706888</td>\n",
       "      <td>@Lissetteeee im fixing up myspace &amp;amp; facebo...</td>\n",
       "      <td>67</td>\n",
       "      <td>@Lissetteeee im fixing up myspace &amp;amp; facebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13434</th>\n",
       "      <td>1553239046</td>\n",
       "      <td>i lost my cell phone.     gunna go hangout wit...</td>\n",
       "      <td>97</td>\n",
       "      <td>i lost my cell phone.     gunna go hangout wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  length  \\\n",
       "701    1467986323  Re-pinging @Kyle44: Custom icons I made! =] lo...     125   \n",
       "2889   1468472273  @swati121 SWATI!!! omg i missed you soo much  ...     125   \n",
       "10129  1550780676       its a sunny day today yay =] but still cold       44   \n",
       "12206  1551706888  @Lissetteeee im fixing up myspace &amp; facebo...      67   \n",
       "13434  1553239046  i lost my cell phone.     gunna go hangout wit...      97   \n",
       "\n",
       "                                              emoji_text  \n",
       "701    Re-pinging @Kyle44: Custom icons I made! _emo_...  \n",
       "2889   @swati121 SWATI!!! omg i missed you soo much  ...  \n",
       "10129  its a sunny day today yay _emo_pos_ but still ...  \n",
       "12206  @Lissetteeee im fixing up myspace &amp; facebo...  \n",
       "13434  i lost my cell phone.     gunna go hangout wit...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents containing negative emoji: 319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1468072508</td>\n",
       "      <td>@_saffron  Why not? :[</td>\n",
       "      <td>22</td>\n",
       "      <td>@_saffron  Why not? _emo_neg_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9532</th>\n",
       "      <td>1548781156</td>\n",
       "      <td>@billbeckett a couple of sad faces...   =[  or...</td>\n",
       "      <td>89</td>\n",
       "      <td>@billbeckett a couple of sad faces...   =[  or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16363</th>\n",
       "      <td>1555894750</td>\n",
       "      <td>RAWR! Piano is evil!!! Im better at sports; i ...</td>\n",
       "      <td>111</td>\n",
       "      <td>RAWR! Piano is evil!!! Im better at sports; i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21333</th>\n",
       "      <td>1557257172</td>\n",
       "      <td>Omfg miss my internet :'( fuck my bro &amp;gt;:[ I...</td>\n",
       "      <td>72</td>\n",
       "      <td>Omfg miss my internet :'( fuck my bro &amp;gt;_emo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33163</th>\n",
       "      <td>1564434250</td>\n",
       "      <td>@Halo_Kitteh :[ ... i dont understand why it k...</td>\n",
       "      <td>138</td>\n",
       "      <td>@Halo_Kitteh _emo_neg_ ... i dont understand w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  length  \\\n",
       "1063   1468072508                             @_saffron  Why not? :[      22   \n",
       "9532   1548781156  @billbeckett a couple of sad faces...   =[  or...      89   \n",
       "16363  1555894750  RAWR! Piano is evil!!! Im better at sports; i ...     111   \n",
       "21333  1557257172  Omfg miss my internet :'( fuck my bro &gt;:[ I...      72   \n",
       "33163  1564434250  @Halo_Kitteh :[ ... i dont understand why it k...     138   \n",
       "\n",
       "                                              emoji_text  \n",
       "1063                       @_saffron  Why not? _emo_neg_  \n",
       "9532   @billbeckett a couple of sad faces...   =[  or...  \n",
       "16363  RAWR! Piano is evil!!! Im better at sports; i ...  \n",
       "21333  Omfg miss my internet :'( fuck my bro &gt;_emo...  \n",
       "33163  @Halo_Kitteh _emo_neg_ ... i dont understand w...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"documents containing positive emoji:\",len(documents[documents['emoji_text'].str.contains(\"_emo_pos_\")]))\n",
    "display(documents[documents['emoji_text'].str.contains(\"_emo_pos_\")].head())\n",
    "print(\"documents containing negative emoji:\",len(documents[documents['emoji_text'].str.contains(\"_emo_neg_\")]))\n",
    "display(documents[documents['emoji_text'].str.contains(\"_emo_neg_\")].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80548565-bbc1-4917-b665-90b594095b46",
   "metadata": {},
   "source": [
    "Remark : we do not do any work on emojis and emoticons here, but this could be a job for a real-life documents analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd319d65",
   "metadata": {},
   "source": [
    "# The NLP pipeline\n",
    "\n",
    "In this notebook, because we run pipelines for the only purpose to run and benchmark pipelines, we do not really have to build a robust and efficient pipeline covering all NLP aspects! For this reason, we will focus on a few regex transformations, and POS Tagging + Lemmatizing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e15379ee-19ab-4e7f-841c-9b8c878a1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -----------------------------------------------------------------------------------\n",
    "## NLP PREPROCESSING FUNCTIONS\n",
    "## Here is a somehow basic pipeline with regex and POS tagging + lemmatizing\n",
    "## emojis and emoticons are not handled\n",
    "\n",
    "verbose_level = 0 # set to 1 or greater to be more verbose\n",
    "\n",
    "### Some Regex\n",
    "\n",
    "def re_linebreaks(textes): \n",
    "    global verbose_level\n",
    "    t0 = time()\n",
    "    result = [re.sub('[\\n\\r]', ' ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp line breaks removal\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "def re_hyperlinks(textes): \n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'  \n",
    "    result = [re.sub(pattern, ' _LINK_ ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp Hyperlinks patching\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "def re_dates(textes): \n",
    "    global verbose_level\n",
    "    t0 = time()\n",
    "    pattern = '\\d+[\\.\\/-]\\d+[\\.\\/-]\\d+' \n",
    "    result = [re.sub(pattern, ' _DATE_ ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp Dates patching\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "\n",
    "def re_money(textes): \n",
    "    global verbose_level\n",
    "    t0 = time()\n",
    "    pattern = '\\$[ ]?\\d+((,|\\.)\\d+)?|\\d+((,|\\.)\\d+)?[ ]?\\$' \n",
    "    result = [re.sub(pattern, ' _MONEY_ ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp money patching\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "def re_numbers(textes): \n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    result = [re.sub('\\d+((,|\\.)\\d+)?', ' _NUMBER_ ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp numbers patching\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    " \n",
    "\n",
    "def re_ordinals(textes): \n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    result = [re.sub('\\d+((,|\\.)\\d+)?(st|nd|rd|th)', ' _ORDINAL_ ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp ordinals patching\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "\n",
    "def re_arobace_usernames(textes): \n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    pattern = '([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)' # @user123_name \n",
    "    result = [re.sub(pattern, ' _USER_ ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp arobace_usernames patching\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "def re_hashtags(textes): \n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    pattern = '#\\w*' #  #whatever \n",
    "    result = [re.sub(pattern, ' _HASHTAG_ ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp hashtag patching\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def re_special_chars_except_punctuation(textes): \n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    result = [re.sub('[^\\w.,;:!\\?\\-\\' \\n]+', ' ', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp special chars except punctuation cleaning\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "\n",
    "def re_special_chars(textes): \n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    \n",
    "    result = [re.sub(\"[^\\u0000-\\u05C0\\u2100-\\u214F]+\",\"\",t) for t in textes] #non occidental characters \n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp special chars cleaning\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "def re_remove_htmlentities(textes):\n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    \n",
    "    result = [re.sub('\\&\\w*;',\"\",t) for t in textes] # &whatever;\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp HTML entities removal\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "    \n",
    "\n",
    "\n",
    "def re_repeating_punctuation(textes): \n",
    "    global verbose_level\n",
    "    t0 = time()  \n",
    "    result = [re.sub(r'([.,;:!\\?\\-\\'])\\1*',r'\\1', t) for t in textes]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp repeating punctuation chars cleaning\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def re_whitespaces(textes): \n",
    "    global verbose_level\n",
    "    t0 = time()  \n",
    "    white_spaces = [re.sub(r'[ \\t]{2,}', ' ', t) for t in textes]\n",
    "#     white_spaces = [re.sub('[ \\t]+', ' ', t) for t in textes]\n",
    "    result = [re.sub('[ \\t]+$', '', r) for r in white_spaces]       \n",
    "    if (verbose_level>0):\n",
    "        print(\"Regexp white spaces cleaning\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return result   \n",
    "\n",
    "\n",
    "def apply_regex_transformers(funclist, textes,max_examples=0):\n",
    "    global verbose_level\n",
    "    \n",
    "    if verbose_level>0:\n",
    "        max_examples=1\n",
    "        \n",
    "    input_text = list(textes)\n",
    "    for fname , f in funclist.items():\n",
    "        if (max_examples>0):\n",
    "            print(\"\")\n",
    "            print(\"------- \"*6)\n",
    "            print(\"------- \"+fname+\" example \" + \" ------- \"*3)\n",
    "        \n",
    "        output_text=f(input_text)\n",
    "        \n",
    "        if (max_examples>0):\n",
    "            count_samples=0\n",
    "            print(\"------- \"*6)\n",
    "            for i in range(len(input_text)):\n",
    "                if (output_text[i]!=input_text[i]):\n",
    "                    count_samples = count_samples+1\n",
    "                    print(\"--- Before : \\n\"+input_text[i])\n",
    "                    print(\"--- After : \\n\"+output_text[i])\n",
    "                    print()\n",
    "                if count_samples==max_examples: \n",
    "                    break \n",
    "        input_text= output_text \n",
    "    return output_text\n",
    "\n",
    "def get_regex_transformers():\n",
    "    regex_transformers = {\n",
    "        'newlines': re_linebreaks,\n",
    "        'hyperlinks': re_hyperlinks,\n",
    "        'dates': re_dates,\n",
    "        'arobace usernames': re_arobace_usernames,\n",
    "        'money': re_money,\n",
    "        'ordinals': re_ordinals, \n",
    "        'numbers': re_numbers,  \n",
    "        'hashtags':re_hashtags,\n",
    "        'HTML entities removal':re_remove_htmlentities,\n",
    "        'repeating punctuation chars':re_repeating_punctuation,\n",
    "        'special_chars_except_punctuation': re_special_chars_except_punctuation,\n",
    "        'special_chars': re_special_chars,\n",
    "        'whitespaces': re_whitespaces\n",
    "    } \n",
    "    return regex_transformers\n",
    " \n",
    "def sentences_tokenizer(text_list):\n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "     \n",
    "    # NLTK >=3.6.7 is required.\n",
    "    # see https://github.com/nltk/nltk/issues/2927 \n",
    "    # this call leads to a bug when the tweet starts with a '.'\n",
    "    # issue from NLTK 3.6.6 (See #2925, #2921), and has been patched in NLTK 3.6.7\n",
    "    sentences_list = [nltk.sent_tokenize(clean_text) if len(clean_text)>0 else [] for clean_text in text_list]\n",
    "    \n",
    "    if (verbose_level>0):\n",
    "        print(\"Sentences tokenizer\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return sentences_list\n",
    " \n",
    "\n",
    "# version qui tokenize,POS-tag en 1 passage avec spacy\n",
    "def sentences_word_token_POS(sent_list):\n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    # approche spacy\n",
    "    # input = liste de documents, eux mêmes des listes de phrases\n",
    "    # output = liste de documents, découpés en liste de phrases, découpées en listes de paires (lemma,POS-tag)\n",
    "    nlp = spacy.load(\"en_core_web_sm\",exclude=['lemmatizer','parser','ner'])\n",
    "    \n",
    "    special_tags=[\"_LINK_\",\"_DATE_\",\"_NUMBER_\",\"_MONEY_\",\"_ORDINAL_\",\"_USER_\",\"_HASHTAG_\"]\n",
    "    for special_tag in special_tags:\n",
    "        nlp.tokenizer.add_special_case(special_tag, [{\"ORTH\": special_tag}])\n",
    "    # on remplace les tags par une forme orthographique (par ex _LINK_ devient LINK ) après POS tagging, pour permettre un traitement deep learning plus pertinent dès le plongement\n",
    "    sentences_token_POS_list=[[[(st.text.replace('_','') , st.pos_) for st in nlp(s)] for s in sent ] for sent in sent_list] \n",
    "    \n",
    "    if (verbose_level>0):\n",
    "        print(\"Sentences spacy tokenizing+POS chain\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return sentences_token_POS_list\n",
    "\n",
    "\n",
    "\n",
    "# version qui tokenize,POS-tag, lemmatize en 1 passage avec spacy\n",
    "def sentences_word_token_POS_lemma(sent_list):\n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    # approche spacy\n",
    "    # input = liste de documents, eux mêmes des listes de phrases\n",
    "    # output = liste de documents, découpés en liste de phrases, découpées en listes de paires (lemma,POS-tag)\n",
    "    nlp = spacy.load(\"en_core_web_sm\",exclude=['parser','ner'])\n",
    "    \n",
    "    special_tags=[\"_LINK_\",\"_DATE_\",\"_NUMBER_\",\"_MONEY_\",\"_ORDINAL_\",\"_USER_\",\"_HASHTAG_\"]\n",
    "    for special_tag in special_tags:\n",
    "        nlp.tokenizer.add_special_case(special_tag, [{\"ORTH\": special_tag}]) \n",
    "    # on remplace les tags par une forme orthographique (par ex _LINK_ devient LINK ) après POS tagging, pour permettre un traitement deep learning plus pertinent dès le plongement\n",
    "    sentences_lemma_POS_list=[[[(st.lemma_ , st.pos_) if st.text[0]!=\"_\" else (st.text.replace('_','') , st.pos_) for st in nlp(s)] for s in sent ] for sent in sent_list] \n",
    "    \n",
    "    if (verbose_level>0):\n",
    "        print(\"Sentences spacy tokenizing+POS+lemmatizing chain\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return sentences_lemma_POS_list\n",
    "\n",
    " \n",
    "\n",
    "def sentence_lowercase(sent_POS_list):\n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    sentences_filtered_list = []\n",
    "    for sent_POS in sent_POS_list:\n",
    "        sentences_filtered = []\n",
    "        for s in sent_POS: \n",
    "    #         print(\"parsing\",s)\n",
    "            s = [(t.lower(),pos) for t,pos in s]\n",
    "            sentences_filtered = sentences_filtered + [s]\n",
    "        sentences_filtered_list = sentences_filtered_list + [sentences_filtered]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Sentences lowercase filtering\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return sentences_filtered_list\n",
    "\n",
    "def sentence_depunctuate(sent_POS_list):\n",
    "    global verbose_level\n",
    "    punctuations = set([',','.',';',':','!','?','-','...'])\n",
    "    t0 = time() \n",
    "    sentences_filtered_list = []\n",
    "    for sent_POS in sent_POS_list:\n",
    "        sentences_filtered = []\n",
    "        for s in sent_POS: \n",
    "    #         print(\"parsing\",s)\n",
    "            s = [(t,pos) for t,pos in s if t not in punctuations]\n",
    "            sentences_filtered = sentences_filtered + [s]\n",
    "        sentences_filtered_list = sentences_filtered_list + [sentences_filtered]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Sentences depunctuation\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return sentences_filtered_list\n",
    "\n",
    " \n",
    "\n",
    "def sentence_clean_empty(sent_POS_list):\n",
    "    global verbose_level\n",
    "    t0 = time() \n",
    "    sentences_filtered_list = []\n",
    "    for sent_POS in sent_POS_list:\n",
    "        sentences_filtered = [s for s in sent_POS if len(s)>0]\n",
    "        sentences_filtered_list = sentences_filtered_list + [sentences_filtered]\n",
    "    if (verbose_level>0):\n",
    "        print(\"Sentences clean empty\",\"done in %0.3fs.\" % (time() - t0))\n",
    "    return sentences_filtered_list\n",
    "\n",
    "\n",
    "def apply_sentence_transformers(funclist, sentences, max_examples=0):\n",
    "    global verbose_level\n",
    "    if verbose_level>0:\n",
    "        max_examples=1\n",
    "    input_sentences = list(sentences)\n",
    "    for fname , f in funclist.items():\n",
    "        if (f.__name__ in [\"sentences_show_POS_stats\",\"sentences_show_frequency_stats\"]):\n",
    "#         if (f == sentences_show_POS_stats): # ne fonctionne pas depuis l'intérieur de la fonction\n",
    "            f(input_sentences,fname)\n",
    "        else:\n",
    "            if (max_examples>0):\n",
    "                print(\"\")\n",
    "                print(\"------- \"*6)\n",
    "                print(\"------- \"+fname+\" example \" + \" ------- \"*3)\n",
    "\n",
    "            output_sentences=f(input_sentences)\n",
    "\n",
    "            if (max_examples>0):\n",
    "                count_samples=0\n",
    "                print(\"------- \"*6)\n",
    "                for i in range(len(input_sentences)):\n",
    "                    if (output_sentences[i]!=input_sentences[i]):\n",
    "                        count_samples = count_samples+1\n",
    "                        print(\"--- Before :\")\n",
    "                        print(input_sentences[i])\n",
    "                        print(\"--- After :\")\n",
    "                        print(output_sentences[i])\n",
    "                        print()\n",
    "                    if count_samples==max_examples: \n",
    "                        break\n",
    "                # fin boucle d'affichage des exemples\n",
    "            # fin si affichage exemples\n",
    "            input_sentences= output_sentences\n",
    "    # fin boucle \n",
    "    return output_sentences\n",
    "\n",
    "\n",
    "# token_transforming_method permet de choisir entre\n",
    "# - le token original : \"token\"\n",
    "# - le token lemmatisé : \"lemma\" \n",
    "def get_sentence_transformers(token_transforming_method=\"lemma\"): \n",
    "    if token_transforming_method==\"token\":\n",
    "        sentence_transformers = {\n",
    "            'sentences tokenizer':sentences_tokenizer,\n",
    "            'sentences words tokenizer + POS-tagger':sentences_word_token_POS, \n",
    "            'case':sentence_lowercase,\n",
    "            'depunctuation': sentence_depunctuate,\n",
    "            \"remove empty sentences\":sentence_clean_empty\n",
    "        }\n",
    "    elif token_transforming_method==\"lemma\":\n",
    "        sentence_transformers = {\n",
    "            'sentences tokenizer':sentences_tokenizer,\n",
    "            'sentences words tokenizer + POS-tagger + lemmatizer':sentences_word_token_POS_lemma, \n",
    "            'case':sentence_lowercase,\n",
    "            'depunctuation': sentence_depunctuate,\n",
    "            \"remove empty sentences\":sentence_clean_empty\n",
    "        } \n",
    "    return sentence_transformers \n",
    "                \n",
    "\n",
    "\n",
    "class RegexTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, regex_transformers):   \n",
    "        self.regex_transformers = regex_transformers \n",
    "\n",
    "    def fit(self, X, y = None): \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None): \n",
    "        return apply_regex_transformers(self.regex_transformers,X) \n",
    "\n",
    "    \n",
    "class SentenceTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sentence_transformers):   \n",
    "        self.sentence_transformers = sentence_transformers \n",
    "\n",
    "    def fit(self, X, y = None): \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None): \n",
    "        return apply_sentence_transformers(self.sentence_transformers,X)     \n",
    "\n",
    "# Building the Pipeline \n",
    "def get_preprocessing_text_pipeline(token_transforming_method=\"lemma\"):\n",
    "    text_pipeline = Pipeline([\n",
    "        ('regex', RegexTransformer(get_regex_transformers())),\n",
    "        ('sentences', SentenceTransformer(get_sentence_transformers(token_transforming_method)))\n",
    "    ])\n",
    "    return text_pipeline\n",
    "\n",
    "def get_unique_tokens_in_POS_list(sentence_POS_list):  \n",
    "    data=[]\n",
    "    data = [pair[0] for sent_POS in sentence_POS_list for sent in sent_POS for pair in sent]\n",
    "    return set(data)\n",
    "    \n",
    "def get_unique_tokens_in_list(doc_list):  \n",
    "    data=[]\n",
    "    data = [t for doc in doc_list for t in doc]\n",
    "    return set(data)\n",
    "    \n",
    "def flatten_list(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "\n",
    "def get_unsentenced_unPOStagged_list(sentence_POS_list):\n",
    "    return [untag(flatten_list(doc)) for doc in sentence_POS_list]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22f5c561-2816-4e49-821e-5f86e5ed0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP pipeline\n",
    "# Building the tokenizing/lemmatizing Pipeline\n",
    "text_pipeline = get_preprocessing_text_pipeline()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815b040",
   "metadata": {},
   "source": [
    "## Testing NLP Pipeline on a several corpus sizes\n",
    "\n",
    "We want to test here the NLP pipeline on a large corpus, to get some insights about the pipeline complexity when the corpus size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf6e0f-86da-4ec3-b498-04d9953a88c6",
   "metadata": {},
   "source": [
    "### Big size, big time\n",
    "\n",
    "First of all, we would like to know the pipeline execution time for big corpus sizes, so that we can later approximate this tendency and predict accurate times for very large corpus sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70185110-5cb5-4978-bc94-4a3f97aa7795",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_benchmark_nobatch_file = dataset_path+\"NLP_benchmark_nobatch.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4240b60d-66ef-47a7-84a9-d0e381901d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes=[10,100,1000,5000,10000,20000,40000,80000,120000,160000,250000,320000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b3bb1-e2be-4bf0-aa75-d8e6dca3924d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark for N= 10\n",
      "... Time = 0.8555445671081543 s = 0.00023765126864115396 h\n",
      "Benchmark for N= 100\n",
      "... Time = 1.3205547332763672 s = 0.00036682075924343534 h\n",
      "Benchmark for N= 1000\n",
      "... Time = 7.150355577468872 s = 0.0019862098826302424 h\n",
      "Benchmark for N= 5000\n",
      "... Time = 33.25322484970093 s = 0.009237006902694703 h\n",
      "Benchmark for N= 10000\n",
      "... Time = 67.17784237861633 s = 0.01866051177183787 h\n",
      "Benchmark for N= 20000\n",
      "... Time = 139.67915034294128 s = 0.03879976398415036 h\n",
      "Benchmark for N= 40000\n"
     ]
    }
   ],
   "source": [
    "if not exists(NLP_benchmark_nobatch_file):\n",
    "    NLP_benchmark= pd.DataFrame(columns=[\"size\",\"time\"])\n",
    "    verbose_level=0\n",
    "    for N in sample_sizes:\n",
    "        df_sample=documents.sample(N,random_state=618).copy()\n",
    "        t0=time()\n",
    "        print(\"Benchmark for N=\",N)\n",
    "        X = df_sample[\"text\"].tolist()  \n",
    "        X_prep=text_pipeline.fit_transform(X)\n",
    "        t=time()-t0\n",
    "        print(\"... Time =\",t,\"s =\",t/3600,\"h\")\n",
    "        NLP_benchmark = NLP_benchmark.append({\"size\":N , \"time\":t},ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72361e84-ca50-4445-ad0d-3db6db5db1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(NLP_benchmark_nobatch_file):\n",
    "    NLP_benchmark = pd.read_pickle(NLP_benchmark_nobatch_file)\n",
    "else :\n",
    "    NLP_benchmark.to_pickle(NLP_benchmark_nobatch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f403e-b0ae-4254-8074-b8d7adf1279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,5))\n",
    "plt.xticks(fontsize=13, rotation=90) \n",
    "plt.plot(NLP_benchmark[\"size\"],NLP_benchmark[\"time\"],'bo', linewidth=2, linestyle='dashed', markersize=8,label=\"NLP chain observed time complexity\")  \n",
    "\n",
    "for i,row in NLP_benchmark.iterrows():\n",
    "    if row['size']>10000:\n",
    "        plt.annotate(\"(\"+str(int(row[\"size\"]))+\",\"+str(int(row[\"time\"]))+\"s)\", (row[\"size\"], row[\"time\"]),va=\"bottom\", ha=\"right\") \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"corpus size\")\n",
    "plt.ylabel(\"time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9074c0e-4a42-4c06-a046-fe4284965383",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(NLP_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426453f2-64cc-418b-9dc2-0a6fc7639560",
   "metadata": {},
   "source": [
    "### Small size, small time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0523e-0a9a-4b3f-9055-ba74e1d6875d",
   "metadata": {},
   "source": [
    "When running on small datasets, the pipeline will have a constant time dedicated to setting up its context, structuring variables, calling functions, transmitting parameters. \n",
    "\n",
    "We cannot neglect this constant time, otherwise we could work on a wrong basis when approximating the polynomial coefficients.\n",
    "\n",
    "Let's do some more studies, this time on very small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec2a45-1810-4923-a263-827b66d044bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_small_benchmark_nobatch_file = dataset_path+\"NLP_small_benchmark_nobatch.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab240954-8387-49bc-a0d9-450ab68785d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes=np.arange(1,50,1)\n",
    "nb_tests = 10 # run nb_tests times the same experiment to get mean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a8dcd-1266-42af-b3ed-f94c656e8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(NLP_small_benchmark_nobatch_file):\n",
    "    NLP_small_benchmark= pd.DataFrame(columns=[\"size\",\"time\"])\n",
    "    verbose_level=0\n",
    "    for N in tqdm(sample_sizes):\n",
    "        print(\"Benchmark for N=\",N)\n",
    "        test_time=0\n",
    "        for nb_test in range(0,nb_tests):\n",
    "            df_sample=documents.sample(N).copy()\n",
    "            t0=time()\n",
    "            X = df_sample[\"text\"].tolist()  \n",
    "            X_prep=text_pipeline.fit_transform(X)\n",
    "            t=time()-t0\n",
    "            test_time+=t\n",
    "        t=test_time/nb_tests\n",
    "        print(\"... Time =\",t,\"s =\",t/3600,\"h\")\n",
    "        NLP_small_benchmark = NLP_small_benchmark.append({\"size\":N , \"time\":t},ignore_index=True) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d76a085-4cfc-4a0c-bedd-8a2a74149729",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists(NLP_small_benchmark_nobatch_file):\n",
    "    NLP_small_benchmark = pd.read_pickle(NLP_small_benchmark_nobatch_file)\n",
    "else :\n",
    "    NLP_small_benchmark.to_pickle(NLP_small_benchmark_nobatch_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75933d0-0f58-46cb-9d3c-af51c9d10ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,5))\n",
    "plt.xticks(fontsize=13, rotation=90) \n",
    "plt.plot(NLP_small_benchmark[\"size\"],NLP_small_benchmark[\"time\"],'bo', linewidth=2, linestyle='dashed', markersize=8,label=\"NLP chain observed time complexity on small datasets\")  \n",
    "\n",
    "for i,row in NLP_small_benchmark.iterrows():\n",
    "    # if row['size']<1000:\n",
    "    plt.annotate(\"(\"+str(int(row[\"size\"]))+\",\"+str(round(row[\"time\"],2))+\"s)\", (row[\"size\"], row[\"time\"]),va=\"bottom\", ha=\"right\") \n",
    "plt.legend()\n",
    "plt.xlabel(\"corpus size\")\n",
    "plt.ylabel(\"time (s)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154fec8d-0fb3-4821-80d5-864211dc399a",
   "metadata": {},
   "source": [
    "## Understand the pipeline complexity\n",
    "\n",
    "Looking at the pipeline execution times on several corpus sizes, we can state that the pipeline complexity is not linear, but rather shows a quadratic tendency.\n",
    "\n",
    "\n",
    "With the NLP libraries used in the pipeline, and the poor available documentation about their computational complexity, it is quite impossible to express precisely the pipeline processing time as a function of the corpus size. Moreover, the complexity depends on other factors such as sentences lengh and each parameter used to setup each pipeline step.\n",
    "\n",
    "However with the observations we just made on various sampled subcorpora, we can approximate the pipeline complexity as a polynomial function of the corpus size. \n",
    "\n",
    "We will consider that this complexity is **at least quadratic** ( >= O(N²) ). Note that this is just a work hypothesis.\n",
    "\n",
    "If the actual pipeline complexity appeared to be a polynomial of higher order (for instance O(N^3)), the optimizations computed with our method would still be efficient and would even allow a better computational economy.\n",
    "\n",
    "If the actual pipeline tendency was not quadratic but a linear (with very big coefficient) or a composition of N and log(N) with high coefficients, our study would remain efficient as far as it is based on dividing N into smaller chunks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2637c-053e-49fa-a9af-da7400f76d44",
   "metadata": {},
   "source": [
    "### quadratic approximation of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae956657-9cab-4e76-b089-299e14bbf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group our benchmarks to have a full scope\n",
    "max_small_sizes=NLP_small_benchmark['size'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ea4ce-f159-47ea-aba3-325a2ce9a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_benchmark=pd.concat([NLP_small_benchmark,NLP_benchmark[NLP_benchmark['size']>max_small_sizes]],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415ee31-ac9d-4455-a4bf-ca3974a88103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.polynomial as nppol\n",
    "\n",
    "plt.figure(figsize=(24,5))\n",
    "plt.xticks(fontsize=13, rotation=90) \n",
    "plt.plot(NLP_benchmark[\"size\"],NLP_benchmark[\"time\"],'bo--', linewidth=2, markersize=8,label=\"NLP pipeline observed time complexity\")  \n",
    "\n",
    "max_x= len(documents)/8 # manual_NLP_benchmark[\"size\"].max()\n",
    "\n",
    "# ----------- approximation de la courbe de complexité en temps\n",
    "df_approx=pd.DataFrame(columns=[\"x\"])\n",
    "line_x = np.arange(0,max_x+1,100)\n",
    "df_approx[\"x\"]=line_x\n",
    " \n",
    "# ----------- (quadratic) polynomial regression with Numpy\n",
    "pp , [resid, rank, sv, rcond] = nppol.Polynomial.fit(NLP_benchmark[\"size\"],NLP_benchmark[\"time\"], 2, full=True) # quadratic regression\n",
    "# see https://numpy.org/doc/stable/reference/generated/numpy.polynomial.polynomial.Polynomial.fit.html#numpy.polynomial.polynomial.Polynomial.fit\n",
    "# pp will contain our coefficients\n",
    "# resid is the residual value =  sum of the squares of the fit errors\n",
    "# a high residual value can be due to statistical noise, or poor fitting\n",
    "\n",
    "# ppc=pp.coef\n",
    "ppcc=pp.convert().coef\n",
    "# print(\"Polynomial.fit coefficients for ax²+bx+c : (a,b,c)=\",ppc[2],ppc[1],ppc[0])\n",
    "print(\"Polynomial.fit coefficients for ax²+bx+c : (a,b,c)=\",ppcc[2],ppcc[1],ppcc[0])\n",
    "print(\"Coefficients = \",ppcc)\n",
    "print(\"Residual = \",resid)\n",
    "# df_approx[\"reg_y\"] = df_approx[\"x\"].apply(lambda x: ppc[2]*(x**2) + ppc[1]*x + ppc[0])\n",
    "# plt.plot(df_approx[\"x\"], df_approx[\"reg_y\"],  color='purple', alpha=.7, linewidth=2, linestyle='dashed', label=\"Parabolic time computed regression\")\n",
    "df_approx[\"reg_cy\"] = df_approx[\"x\"].apply(lambda x: ppcc[2]*(x**2) + ppcc[1]*x + ppcc[0])\n",
    "plt.plot(df_approx[\"x\"], df_approx[\"reg_cy\"], color='purple',alpha=.7, linewidth=2, linestyle='dashed', label=\"Parabolic time computed regression\")  \n",
    " \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size (nb documents)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5c66b8-e5b1-4427-8d1b-71167ebd18be",
   "metadata": {},
   "source": [
    "### constant term correction in the polynomial approximation\n",
    "\n",
    "Now let's have a closer look at the polynomial approximation for small corpus sizes.\n",
    "\n",
    "We observe that the constant term 'c' is negative. This does not account for the values we found on small datasets. We have to correct this constant term, otherwise our optimization process will give us a bad optimal batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be3650-7fb9-48fb-a50f-d581f50397b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The polynomial constant term is \",ppcc[0])\n",
    "print(\"The minimal value of our obseved times is \",NLP_small_benchmark[\"time\"].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052f600a-38a8-46bb-918a-be6a90bbf229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.polynomial.polynomial as nppol\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.xticks(fontsize=13, rotation=90) \n",
    "plt.plot(NLP_small_benchmark[\"size\"],NLP_small_benchmark[\"time\"],'bo', linewidth=2, markersize=8,label=\"NLP pipeline observed time complexity on small datasets\")  \n",
    "\n",
    "max_x= int(NLP_small_benchmark[\"size\"].max())+5 # manual_NLP_benchmark[\"size\"].max()\n",
    "\n",
    "# ----------- approximation de la courbe de complexité en temps\n",
    "df_approx=pd.DataFrame(columns=[\"x\"])\n",
    "line_x = np.arange(0,max_x+1,1)\n",
    "df_approx[\"x\"]=line_x\n",
    "  \n",
    "df_approx[\"reg_cy\"] = df_approx[\"x\"].apply(lambda x: ppcc[2]*(x**2) + ppcc[1]*x + ppcc[0])\n",
    "plt.plot(df_approx[\"x\"], df_approx[\"reg_cy\"],  color='purple', alpha=.7, linewidth=2, linestyle='dashed', label=\"Parabolic time computed regression (without constant term correction)\")  \n",
    "corrected_term=NLP_small_benchmark[\"time\"].min()\n",
    "df_approx[\"reg_ccy\"] = df_approx[\"x\"].apply(lambda x: ppcc[2]*(x**2) + ppcc[1]*x + corrected_term)\n",
    "plt.plot(df_approx[\"x\"], df_approx[\"reg_ccy\"],  color='green', alpha=.7, linewidth=2, linestyle='dashed', label=\"Parabolic time computed regression (with constant term correction)\")  \n",
    " \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size (nb documents)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0fb1ca-4ede-4e72-86a7-c94cadfa813f",
   "metadata": {},
   "source": [
    "So finally we correct the constant term of our polynomial approximation, for more accurate approximations in the following study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443fa17-d469-46e0-a45d-261f18854a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not want the curve to move under 0 so we force the coefficient c=0 if c<0\n",
    "if (ppcc[0]<corrected_term):\n",
    "    ppcc[0]=corrected_term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95caca7",
   "metadata": {},
   "source": [
    "**General expression of our pipeline complexity**\n",
    "\n",
    "With all the experiments done so far, we have computed a polynomial expression of the pipeline time, given our computer hardware.\n",
    "\n",
    "    Time(Pipeline(N)) = a x N² + b x N + c\n",
    "    \n",
    "We can now estimate the computation time for any value of N.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91837f85-bff5-45d5-81ff-7b4678be967b",
   "metadata": {},
   "source": [
    "### predict computation time for larger corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288cc39-40a1-459f-a4bc-76066ee698bf",
   "metadata": {},
   "source": [
    "Now that we have polynomial coefficients, we can estimate the computation time for any corpus size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d06405f-6274-47f3-944f-6996fd61ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(documents)\n",
    "\n",
    "NLP_benchmark_predictions= pd.DataFrame(columns=[\"size\",\"time\"])\n",
    "prediction_sizes=[N//8,N//7,N//6,N//5,N//4,N//3,N//2,N]\n",
    "for n in prediction_sizes:\n",
    "    estim_time=ppcc[2]*(n**2) + ppcc[1]*n + ppcc[0]\n",
    "    print(\"estimated time (\",n,\" documents) =\",estim_time,\"s =\",estim_time/3600,\"h\")\n",
    "    NLP_benchmark_predictions = NLP_benchmark_predictions.append({\"size\":n , \"time\":estim_time},ignore_index=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11f340-b3cb-4c14-bbc6-93dd145c2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.xticks(fontsize=13, rotation=90) \n",
    "\n",
    "plt.plot(NLP_benchmark[\"size\"],NLP_benchmark[\"time\"],'bo', linewidth=2, markersize=8,label=\"NLP pipeline observed time complexity\")  \n",
    "\n",
    "plt.plot(NLP_benchmark_predictions[\"size\"],NLP_benchmark_predictions[\"time\"],'ro', linewidth=2, markersize=8,label=\"NLP pipeline estimated quadratic time\")  \n",
    "\n",
    "max_x= len(documents) # manual_NLP_benchmark[\"size\"].max()\n",
    "\n",
    "# ----------- approximation de la courbe de complexité en temps\n",
    "df_approx=pd.DataFrame(columns=[\"x\"])\n",
    "line_x = np.arange(0,max_x+1,1000)\n",
    "df_approx[\"x\"]=line_x\n",
    " \n",
    "# ----------- (quadratic) polynomial regression with Numpy\n",
    " \n",
    "print(\"Polynomial.fit coefficients for ax²+bx+c : (a,b,c)=\",ppcc[2],ppcc[1],ppcc[0])\n",
    "print(\"Coefficients = \",ppcc)\n",
    "print(\"Residual = \",resid) \n",
    "df_approx[\"reg_cy\"] = df_approx[\"x\"].apply(lambda x: ppcc[2]*(x**2) + ppcc[1]*x + ppcc[0])\n",
    "plt.plot(df_approx[\"x\"], df_approx[\"reg_cy\"],  color='purple', alpha=.7, linewidth=2, linestyle='dashed', label=\"Parabolic time computed regression\")  \n",
    " \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size (nb documents)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edc3d5",
   "metadata": {},
   "source": [
    "# Optimize time complexity by dividing the corpus into smaller chunks\n",
    "\n",
    "This study is explained in the following blog article : https://tristanv.medium.com/accelerate-nlp-preprocessing-pipeline-by-optimizing-a-batch-processing-loop-cf18dc7ee036\n",
    "\n",
    "We reproduce our method steps hereunder:\n",
    "\n",
    "1. Evaluate the Split+Compute+Merge complexity of the batch-processing loop over a constant pipeline\n",
    "2. Estimate the batch-processing complexity as a function of batch number\n",
    "3. Find the minimum of the batch processing time for a given corpus size\n",
    "4. Run the optimal batch processing loop and verify our estimation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95952e00",
   "metadata": {},
   "source": [
    "## To batch or not to batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5563601",
   "metadata": {},
   "source": [
    "We will now verify experimentally and computationally that **a division of NLP processing operations into several sub-corpuses of reasonable size will have a better complexity than processing the whole documents in 1 pass**.\n",
    "\n",
    "This means that we expect to reduce _Time(Processing(N))_ by dividing *N* into *z* chunks of size *B=N/z*\n",
    "\n",
    "Process N elements gives actually the same result as processing _z_ times _N/z_ elements.\n",
    "\n",
    "Because _Time(Processing(N)) = O(N²)_, we have\n",
    "\n",
    "     z x Time(Processing(N/z)) = z x O(N²/z²) = O(N²/z) \n",
    "     \n",
    "Splitting into _z_ batches therefore allows us to reduce to a complexity that is still quadratic, but _z_ times smaller than on a single batch of size _N_.\n",
    "\n",
    "However, the envisaged division generates an **additional cost** according to the number of batches. Indeed, the following *non-zero* actions must be measured:\n",
    "- Splitting the corpus of size _N_ into _z_ chunks of size _B = N/z_, with _z > 1_, \n",
    "- Storing/assigning the pipeline output of each batch into a place in memory,\n",
    "- Joining the _z_ batches outputs into a variable , to retrieve an equivalent result as with the one-pass operation.\n",
    "\n",
    "Finally our Batch Processing loop will have the following complexity:\n",
    "\n",
    "    Time(Batch_Processing(N)) = Time(Batch_SplitAssignJoin(N,z)) + (z x Time(Processing(N/z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330e2b9e",
   "metadata": {},
   "source": [
    "We will now observe with a graphical studythat the splitting/assigning/joining operations are a linear function of the batch count.\n",
    "\n",
    "    Time(Batch_SplitAssignJoin(N,z)) = O(z) = d x z + e , (with d,e positive constants depending on our hardware configuration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe19235-5358-41ef-98bd-c13b2653f7da",
   "metadata": {},
   "source": [
    "## Evaluate the Split+Compute+Merge complexity of the batch-processing loop over a constant pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a73f2-ea84-43df-b0d0-49e2ecfa16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimation du temps de découpage+assignation+réunion du dataframe \"documents\" de taille N en z lots de taille B = N/z\n",
    "\n",
    "def compute_documents_batch_split_assign_join_complexity():\n",
    "    documents = pd.read_pickle(documents_initial_working_file)\n",
    "    documents[\"new_col\"]=\"\" # for our tests\n",
    "    # #test avec un sous ensemble\n",
    "    # # documents=documents.sample(400000).copy()\n",
    " \n",
    "    sample_size = len(documents)\n",
    "    \n",
    "    def eval_batchtime(df,batch_size):\n",
    "        # compute time for split+assign+merge on a batch processing over a dataframe df\n",
    "        t0=time() \n",
    "        nb_batch=0\n",
    "        new_col=[]\n",
    "        for i in range(0,df.shape[0],batch_size):\n",
    "            nb_batch=nb_batch+1\n",
    "            t1=time()\n",
    "            \n",
    "            df_batch = df[i:i+batch_size] # split\n",
    "            \n",
    "            # our simple pipeline\n",
    "            X = df_batch[\"text\"].tolist()\n",
    "            \n",
    "            X_pred=X # a simple *innocent* assignment\n",
    "            \n",
    "            new_col.extend(X_pred) # consolidation\n",
    "            \n",
    "        # end batch loop\n",
    "        \n",
    "        df[\"new_col\"]=new_col # final assignment\n",
    "        batchtime=time()-t0\n",
    "        return batchtime\n",
    " \n",
    "    df_batchtime=pd.DataFrame(columns=[\"test\",\"nb_batches\",\"batch_size\",\"time\"])\n",
    "    nb_batches=[1,2,3,4,5,6,7,8,9,10,15,20,25,50,75,100,150,200,250,500,1000,5000,10000,20000,40000,60000]\n",
    "    nb_tests=3\n",
    "    print(\" --- \"*10,\"Computing batch times\")\n",
    "    for test in tqdm(range(0,nb_tests)):\n",
    "        # for each test loop we shuffle the dataframe, so that we get various situations \n",
    "        # to shuffle the dataframe in-place and reset the index without creating new index columns, do drop=True: \n",
    "        documents = documents.sample(frac=1).reset_index(drop=True)\n",
    "        # print(\" --- \"*5,\"Test\",test)\n",
    "        for z in nb_batches:\n",
    "            b=int(round(sample_size/z,0)) \n",
    "            t=eval_batchtime(documents,b)\n",
    "            df_batchtime=df_batchtime.append({\n",
    "                \"test\":test,\n",
    "                \"nb_batches\":z,\n",
    "                \"batch_size\":b,\n",
    "                \"time\":t\n",
    "            },ignore_index=True)\n",
    "    print(\" --- \"*10,\"Finished computing batch times\") \n",
    "    return df_batchtime\n",
    "# end function\n",
    "\n",
    "\n",
    "# inverser les commentaires dans les 4 lignes ci dessous pour exécuter le calcul des stats, ou bien pour le charger depuis un fichier temporaire\n",
    "if (not exists(dataset_path+\"batchtime_splitmerge_eval.pkl\")):\n",
    "    df_batchtime = compute_documents_batch_split_assign_join_complexity()\n",
    "    df_batchtime.to_pickle(dataset_path+\"batchtime_splitmerge_eval.pkl\")\n",
    "else:\n",
    "    df_batchtime = pd.read_pickle(dataset_path+\"batchtime_splitmerge_eval.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b6c61-fe8d-4154-9da9-840e814169da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean=df_batchtime[['nb_batches','batch_size','time']].groupby('nb_batches').mean()\n",
    "df_mean.reset_index(inplace=True)\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd857f-9ca4-4b7e-95a3-e27a236e5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------- linear regression with Numpy\n",
    "lp , [resid, rank, sv, rcond] = nppol.Polynomial.fit(df_mean[df_mean['nb_batches']>=50][\"nb_batches\"],df_mean[df_mean['nb_batches']>=50][\"time\"], 1, full=True) # linear regression \n",
    " \n",
    "lpcc=lp.convert().coef \n",
    "print(\"Polynomial.fit coefficients for dx+e : (d,e)=\",lpcc[1],lpcc[0])\n",
    "print(\"Coefficients = \",lpcc)\n",
    "print(\"Residual = \",resid)\n",
    "df_mean[\"reg_cy\"] = df_mean[\"nb_batches\"].apply(lambda x: lpcc[1]*x + lpcc[0])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3dd49f-3a9b-4dc5-b5ba-bc6c2eb7c430",
   "metadata": {},
   "source": [
    "Let's look at the full curve : complexities appear to follow a linear tendency when batch count is great enough. \n",
    "\n",
    "For small values of batch count, we should look closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc834c82-8f48-4935-b993-add54afabaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,8))\n",
    "plt.title(\"Time to split documents dataframe into batches, assign an array to a batch, and regroup batches, as a function of nb batches\")\n",
    "plt.plot(df_mean[\"nb_batches\"],df_mean[\"time\"],'bo', linestyle='dashed', linewidth=2,label=\"Time for splitting+assign+rejoin DataFrame into batches (value = batch size)\")\n",
    "plt.plot(df_mean[\"nb_batches\"],df_mean[\"reg_cy\"],  color='purple', alpha=.7, linewidth=2, linestyle='dashed', label=\"Linear time computed regression\")  \n",
    "\n",
    "for i,row in df_mean.iterrows():\n",
    "    if row[\"nb_batches\"]>500:\n",
    "        plt.annotate(str(int(row[\"batch_size\"])), (row[\"nb_batches\"], row[\"time\"]),va=\"bottom\", ha=\"right\") \n",
    "plt.legend()\n",
    "plt.xlabel(\"nb batches\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8906af6-3a03-4e5d-8e79-e8dc93509f47",
   "metadata": {},
   "source": [
    "Let's look at the small area when batch count is <500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa924a1d-19e1-4123-a323-53760c0aca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,8))\n",
    "plt.title(\"Time to split documents dataframe into batches, assign an array to a batch, and regroup batches, as a function of nb batches\")\n",
    "df_window=df_mean[df_mean['nb_batches']<=500]\n",
    "plt.plot(df_window[\"nb_batches\"],df_window[\"time\"],'bo', linestyle='dashed', linewidth=2,label=\"Time for splitting+assign+rejoin DataFrame into batches (value = batch size)\")\n",
    "plt.plot(df_window[\"nb_batches\"],df_window[\"reg_cy\"],  color='purple', alpha=.7, linewidth=2, linestyle='dashed', label=\"Linear time computed regression\")  \n",
    "\n",
    "for i,row in df_window.iterrows():\n",
    "    plt.annotate(str(int(row[\"batch_size\"])), (row[\"nb_batches\"], row[\"time\"]),va=\"bottom\", ha=\"right\") \n",
    "plt.legend()\n",
    "plt.xlabel(\"nb batches\")\n",
    "plt.ylabel(\"time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5b06a-75cf-4f96-a814-f7bba885a43a",
   "metadata": {},
   "source": [
    "At this point, we have looked at the extra costs (split/assign/join) implied by the batch processing loop. A few conclusions can be made:\n",
    "- these cost grow as a linear function of the batch number _z_ when _z_ is big enough,\n",
    "- for low values of _z_ the extra cost grows very fast, before reaching the linear slope\n",
    "- all the costs considered when _z_ is low (the fast growth window) are remaining under the ceil of the linear function, and they may be neglected (less than 1.2s) in front of the processing cost itself (several hours when N>1M)\n",
    "\n",
    "For these reasons, we may consider the extra costs as a linear function of the batch number _z_ for all values of _z_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456de1a1-01f7-4a37-9c7c-6cc934fd9890",
   "metadata": {},
   "source": [
    "## Evaluate the full batch processing loop complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adca1a9-69d2-4daf-a892-b74369d47569",
   "metadata": {},
   "source": [
    " \n",
    "Finally, summarizing what we computed so far, we can express the complexity of a batch processing loop over z chunks of size B=N/z :\n",
    "    \n",
    "    Time(Batch_Processing(N,z)) = Time(Batch_SplitAssignJoin(N,z)) + (z x Time(Processing(N/z)))\n",
    "    = (d x z) + e + (z x Time(Processing(N/z))) \n",
    "    = d x z + e + (z x ( a x (N/z)² + b x (N/z) + c)) \n",
    "    = d x z + e + c x z + N x b + a x (N²/z) \n",
    "    = a x (N²/z) +  N x b + (c+d) x z + e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139fea45-9a1a-47b6-aba9-df4ef1128bbe",
   "metadata": {},
   "source": [
    "The complexity of the batch processing loop is a function of two variables : _N_ and _z_ .\n",
    "\n",
    "    Time(Batch_Processing(N,z)) = a x (N²/z) +  N x b + (c+d) x z + e\n",
    "\n",
    "As a polynomial function of _N_, we should observe a parabolic curve growing with a _a/z_ slope.\n",
    "\n",
    "For a given value _N_, the function evolves as an inverse function of _z_ added to a linear function of _z_.\n",
    "\n",
    "Let's look at a few variations of the complexity curve with various values of _z_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb23a22-ed37-4b67-949f-15b2364bc00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the corpus \n",
    "N = len(documents)\n",
    "\n",
    "# coefficients of the pipeline quadratic complexity\n",
    "# Time(Pipeline(N)) = a * N² + b * N + c\n",
    "a = ppcc[2]\n",
    "b = ppcc[1]\n",
    "c = ppcc[0]\n",
    "print(\"Time(Processing(N)) = \",a,\" * N² + \",b,\" * N +\", c)\n",
    "\n",
    "# coefficients of the batch loop split+assign+join linear complexity\n",
    "# Time(Batch_SplitAssignJoin(N,z)) = d * z² + e\n",
    "d = lpcc[1]\n",
    "e = lpcc[0]\n",
    "print(\"Time(Batch_SplitAssignJoin(N,z)) = \",d,\" * z² + \",e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430996e-a829-4972-9cde-931f06e0e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks(fontsize=13, rotation=90) \n",
    "\n",
    "plt.plot(NLP_benchmark[\"size\"],NLP_benchmark[\"time\"],'bo', linewidth=2, markersize=8,label=\"NLP pipeline observed time\")  \n",
    "\n",
    "plt.plot(NLP_benchmark_predictions[\"size\"],NLP_benchmark_predictions[\"time\"],'ro', linewidth=2, markersize=8,label=\"NLP pipeline estimated time\")  \n",
    "\n",
    "max_x= N  \n",
    "\n",
    "# ----------- let's draw several curves with same x values\n",
    "df_approx=pd.DataFrame(columns=[\"x\"])\n",
    "line_x = np.arange(0,max_x+1,1000)\n",
    "df_approx[\"x\"]=line_x\n",
    " \n",
    "# ----------- pipeline quadratic regression\n",
    " \n",
    "df_approx[\"reg_cy\"] = df_approx[\"x\"].apply(lambda x: a*(x**2) + b*x + c)\n",
    "plt.plot(df_approx[\"x\"], df_approx[\"reg_cy\"],  color='purple', alpha=.7, linewidth=2, linestyle='dashed', label=\"Pipeline estimated quadratic regression (no batch)\")  \n",
    "\n",
    "\n",
    "# ----------- several batches\n",
    "\n",
    "z_optimal = int(round(math.sqrt( (a*N*N) / (c+d)),0)) # see how we come to this value further in the notebook\n",
    "nb_batches=[2,3,10,30,z_optimal]\n",
    "\n",
    "for z in nb_batches:\n",
    "    df_approx[\"batch_y\"] = df_approx[\"x\"].apply(lambda x: a*x*x/z +  x*b + (c+d)*z + e)\n",
    "    plt.plot(df_approx[\"x\"], df_approx[\"batch_y\"],  color='green', alpha=.6, linewidth=2, linestyle='dashed', label=\"Batch processing loop estimated time (nb batches = \"+str(z)+\")\")  \n",
    "    last_row = df_approx.iloc[-1]\n",
    "    plt.annotate(\"z=\"+str(z), (last_row[\"x\"], last_row[\"batch_y\"]),va=\"bottom\", ha=\"right\") \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size (nb documents)\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39961364-4f80-4384-96af-dc85a645c681",
   "metadata": {},
   "source": [
    "With a big enough value of *z* (N²/z small enough), we can reduce significantly the processing time.\n",
    "\n",
    "Over this value of _z_, the processing time will start growing again instead of decreasing, because of the extra costs.\n",
    "\n",
    "Let's find out now how to calculate the minimal processing time that we can expect with a given value of N and an optimal value of _z_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ca4d82-fb48-4650-8dc2-4cd9ad375f77",
   "metadata": {},
   "source": [
    "\n",
    "## Calculate the optimal batch count minimizing the batch processing time\n",
    "\n",
    "Let's find a value of *z* such that Time(Batch_Processing(N,z)) is minimal: **the minimum is reached when the derivative of function Time(Batch_Processing(N,z)) with respect to *z* equals 0**:\n",
    "\n",
    "We have :\n",
    "\n",
    "    Time(Batch_Processing(N,z)) = a x (N²/z) +  N x b + (c+d) x z + e\n",
    "    \n",
    "So the derivative is :\n",
    "\n",
    "    deriv( Time(Batch_Processing(N,z)) , z) = -a x N² / z² + (c+d)\n",
    "\n",
    "So we have \n",
    "\n",
    "    deriv( Time(Batch_Processing(N,z)) , z) = 0 \n",
    "    <=> -a x N² / z² + (c+d) = 0\n",
    "    <=> 1 / z² = (c+d) / (a x N²)\n",
    "    <=> z² = (a x N²) / (c+d)\n",
    "    <=> z = square_root( (a x N²) / (c+d))\n",
    "\n",
    "The graphical study here under should show how the full processing time evolves as a function of *z* when *N* is fixed (and big) and confirm this optimal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00cdc5-7cdf-4248-be8a-5105271fde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z_optimal = int(round(math.sqrt( (a*N*N) / (c+d)),0))\n",
    "print(\"We estimate the optimal batch count : z_optimal = sqrt(\",a*N*N,\" / \",(c+d),\") =\",z_optimal)\n",
    "optimal_batch_size=N//z_optimal\n",
    "print(\"The optimal batch size is : N / z_optimal = \",N,\" / \",z_optimal,\" =\",optimal_batch_size)\n",
    "optimal_time = int(a*N*N/z_optimal +  b*N + (c+d)*z_optimal + e)\n",
    "print(\"The optimal time is then : a x (N²/z) +  N x b + (c+d) x z + e = \",optimal_time,\" s = \",(optimal_time/3600),\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec857eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.xticks(fontsize=13, rotation=90) \n",
    "\n",
    "N= len(documents) \n",
    "max_z = N/20 # we 'hope' not to get under chunks of 20 elements\n",
    "\n",
    "# ----------- approximation de la courbe de complexité en temps\n",
    "df_approx=pd.DataFrame(columns=[\"z\"])\n",
    "line_z = np.arange(1,max_z+1,1)\n",
    "df_approx[\"z\"]=line_z\n",
    " \n",
    "df_approx[\"batch_processing_time\"] = df_approx[\"z\"].apply(lambda z: a*N*N/z +  N*b + (c+d)*z + e)\n",
    "df_approx[\"batch_size\"]= N//df_approx[\"z\"]\n",
    "display(\"Minimal computation time can be expected for z=\"+str(z_optimal)+\"\",df_approx[(df_approx[\"z\"]>(z_optimal-5)) & (df_approx[\"z\"]<(z_optimal+5))])\n",
    "\n",
    "plt.axhline(y=(a*N*N +  b*N + c), color='purple', alpha=.7, linewidth=2, linestyle='dashed', label=\"Processing time without batch cut :\"+str(int(a*N*N +  b*N + c))+\" s\") \n",
    "plt.step(df_approx[\"z\"], df_approx[\"batch_processing_time\"], color='blue', alpha=.8, linewidth=2, linestyle='solid', label=\"Processing time with batch cut\") \n",
    "\n",
    "plt.axvline(x=z_optimal, color='k', linestyle='--', label=\"best value for z = \"+str(z_optimal)+\" : time = \"+str(optimal_time)+\"s\")  \n",
    " \n",
    "plt.legend()\n",
    "plt.xlabel(\"Batches count\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2777dfb1",
   "metadata": {},
   "source": [
    "The above study shows that we can drastically accelerate the process thanks to the calculated batch count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86bab0-24a2-463f-85ba-cfab761156f5",
   "metadata": {},
   "source": [
    "## Confirm the estimations with an actual batch processing loop over the full corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f227f-3290-4cd6-b01f-aaf8300b633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reopen the documents dataset prepared earlier for a fresh start\n",
    "# documents_initial_working_file = dataset_path+\"documents_txt.pkl\"\n",
    "documents = pd.read_pickle(documents_initial_working_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ef601",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_NLP_working_file = dataset_path+'clean_documents.pkl'\n",
    "\n",
    "## Batch processing technique\n",
    "nb_batches = z_optimal # see our previous calculation of optimal batch size\n",
    "batch_size = int(round(len(documents)/nb_batches))\n",
    "\n",
    "nb_batch=0\n",
    "verbose_level=0\n",
    "\n",
    "log_file = dataset_path+'batch_log.txt'\n",
    "if exists(log_file):\n",
    "    remove(log_file)\n",
    "\n",
    "logger = open(log_file,\"w\")\n",
    "\n",
    "t0=time()\n",
    "text_prepared=[]\n",
    "print(\" --- \"*5,\"Starting batch process : nb_batches =\",z_optimal,\" batch size=\",batch_size,\" documents size=\",len(documents))\n",
    "logger.write('Starting batch process\\n')\n",
    "for i in range(0,documents.shape[0],batch_size):\n",
    "    nb_batch=nb_batch+1\n",
    "    t1=time()\n",
    "    df_batch = documents[i:i+batch_size] # split\n",
    "    X = df_batch[\"text\"].tolist() \n",
    "    X_prep = text_pipeline.fit_transform(X) # assign\n",
    "    text_prepared.extend(X_prep) # join\n",
    "    logger.write(\"... batch i = \"+str(nb_batch)+\" len(df_batch)=\"+str(len(df_batch))+\" len(X_prep)=\"+str(len(X_prep))+\" time = \"+str(time()-t1)+\" s\\n\")\n",
    "    \n",
    "documents[\"text_POS_prepared\"] = text_prepared # final join\n",
    "\n",
    "t2=time()\n",
    "\n",
    "print(\" --- \"*5)\n",
    "print(\" --- \"*5, \"End of batch process : nb batches = \",nb_batch)\n",
    "print(\"Batch loop done in %0.3fs.\" % (t2 - t0))\n",
    "logger.write(\"End of batch process : nb_batches = \"+str(nb_batch)+\" time = \"+str(t2-t0)+\" s\\n\")\n",
    "\n",
    "documents['terms']=get_unsentenced_unPOStagged_list(documents['text_POS_prepared'])\n",
    "documents['clean_text']=documents['terms'].apply(lambda x : ' '.join(x))\n",
    "\n",
    "display(documents)\n",
    "\n",
    "logger.close()\n",
    "\n",
    "documents.to_pickle(documents_NLP_working_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119bd2d",
   "metadata": {},
   "source": [
    "## in Batch we trust"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801103c-f725-458c-89ec-e6eb884971ed",
   "metadata": {},
   "source": [
    "We can now achieve our work with some conclusions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bb195-b5a6-4762-ac79-7bebd4a644ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The text corpus is composed of\",N,\"documents.\")\n",
    "pipeline_time_seconds=round(a*N*N+b*N+c,0)\n",
    "pipeline_time_hours=round(pipeline_time_seconds/3600,2)\n",
    "print(\"Without batch processing, our pipeline would have been running \",pipeline_time_seconds,\"s or \",pipeline_time_hours,\"h.\")\n",
    "print(\"With our method, we estimated the optimal batch number =\",z_optimal,\"batches. Each batch containing\",optimal_batch_size,\"elements.\")\n",
    "optimal_time_seconds=round(optimal_time,0)\n",
    "optimal_time_hours=round(optimal_time_seconds/3600,2)\n",
    "print(\"Still with this method, we estimated the time needed to batch-process the pipeline =\",optimal_time_seconds,\"s or\",optimal_time_hours,\"hours.\")\n",
    "actual_time_seconds=round(t2-t0,0)\n",
    "actual_time_hours=round(actual_time_seconds/3600,2)\n",
    "print(\"The batch process over our corpus with the optimal batch count was finally executed in\",actual_time_seconds,\"s or\",actual_time_hours,\"hours.\")\n",
    "error_against_optimal_estimation = round(100*abs(actual_time_seconds-optimal_time_seconds)/optimal_time_seconds,2)\n",
    "error_against_nobatch_estimation = round(100*abs(actual_time_seconds-optimal_time_seconds)/pipeline_time_seconds,2)\n",
    "print(\"The difference between batch processing optimal time estimation and actual time is \",abs(actual_time_seconds-optimal_time_seconds),\"s.\")\n",
    "print(\"This difference represents \",error_against_optimal_estimation,\"% of the batch processing estimated time.\")\n",
    "print(\"This difference also represents \",error_against_nobatch_estimation,\"% of the pipeline processing time.\")\n",
    "ratio=round(pipeline_time_seconds/actual_time_seconds,2)\n",
    "print(\"Finally, our method divided the processing time by\",ratio,\".\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f345f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End of notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e28827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
